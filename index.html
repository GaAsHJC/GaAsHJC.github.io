<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Yinglin Zheng</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.5771302b.css" as="style"><link rel="preload" href="/assets/js/app.9223913a.js" as="script"><link rel="preload" href="/assets/js/2.51cc0459.js" as="script"><link rel="preload" href="/assets/js/9.5892c46c.js" as="script"><link rel="preload" href="/assets/js/4.905fbb11.js" as="script"><link rel="preload" href="/assets/js/5.98d9baeb.js" as="script"><link rel="prefetch" href="/assets/js/10.897eb27a.js"><link rel="prefetch" href="/assets/js/11.d9aa9365.js"><link rel="prefetch" href="/assets/js/12.31d18caa.js"><link rel="prefetch" href="/assets/js/13.87f398d1.js"><link rel="prefetch" href="/assets/js/14.e405ab4e.js"><link rel="prefetch" href="/assets/js/15.2f6cc87e.js"><link rel="prefetch" href="/assets/js/16.900ced84.js"><link rel="prefetch" href="/assets/js/17.e70859f8.js"><link rel="prefetch" href="/assets/js/3.13a53d5e.js"><link rel="prefetch" href="/assets/js/6.30799bdd.js"><link rel="prefetch" href="/assets/js/7.dcbd14e0.js"><link rel="prefetch" href="/assets/js/8.0843efa3.js">
    <link rel="stylesheet" href="/assets/css/0.styles.5771302b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" aria-current="page" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Yinglin Zheng</span></a> <div class="links"><!----> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/profile.jpg" alt></div> <div class="info"><div class="name">
      Yinglin Zheng（郑英林）
    </div> <div class="bio"><p>Student at Xiamen University</p></div> <div class="socials"><div><a href="https://github.com/yinglinzheng" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div><div><a href="https://www.linkedin.com/in/%E8%8B%B1%E6%9E%97-%E9%83%91-b17715135/" target="_blank"><img src="/icons/linkedin-mono.svg" alt="linkedin" title="linkedin"></a></div></div> <div class="contact"><div title="Contact me" class="email">zhengyinglin@stu.xmu.edu.cn</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>I'm currently a PhD student in the <a href="https://vcg.xmu.edu.cn/" target="_blank" rel="noopener noreferrer">Visual Computing and Graphics Lab<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> of Xiamen University, supervised by professor <a href="http://mingzeng.xyz/" target="_blank" rel="noopener noreferrer">Ming Zeng<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>I had a fantastic experience as a research intern at Visual Computing Group, <a href="https://www.msra.cn/" target="_blank" rel="noopener noreferrer">Microsoft Research Asia(MSRA)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, working with <a href="https://jianminbao.github.io/" target="_blank" rel="noopener noreferrer">Dr. Jianmin Bao<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="https://www.microsoft.com/en-us/research/people/tinzhan/" target="_blank" rel="noopener noreferrer">Dr. Ting Zhang<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and <a href="http://www.dongchen.pro/" target="_blank" rel="noopener noreferrer">Dr. Dong Chen<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>My research interests lie at human-centric computer vision. I serve as the reviewer of CVPR, ICCV, ECCV, AAAI.</p> <h2 id="news">News</h2> <ul><li>[Feb. 2023] Our work MaskCLIP is accepted by CVPR 2023.</li> <li>[Dec. 2022] Our work on singing head generation has been accepted by CVM2023 and will be published on CVMJ.</li> <li>[Nov. 2022] Our work on multi-exposure image fusion named EMEF has been accepted by AAAI2023.</li> <li>[Oct. 2022] I and my roommate <a href="https://wenjindeng.netlify.app/" target="_blank" rel="noopener noreferrer">Wenjin Deng<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> won the National Scholarship for Postgraduate Students at the same time.</li> <li>[Sep. 2022] My tutorial about <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Denoising Diffusion Probabilistic Models<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is available on <a href="https://www.bilibili.com/video/BV1rW4y1Y7M5/" target="_blank" rel="noopener noreferrer">Bilibili<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[Jun. 2022] The <a href="https://github.com/FacePerceiver/LAION-Face" target="_blank" rel="noopener noreferrer">LAION-Face<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> dataset was released to support large-scale face pretraining.</li> <li>[Jun. 2022] I received the reward of Excellent from the Star of Tomorrow Internship program in Microsoft Research Asia.</li> <li>[Apr. 2022] Our work on multi-person pose estimation named <strong>I^2R-Net</strong> is accepted by IJCAI 2022.</li> <li>[Mar. 2022] <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">FaRL<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is accepted by CVPR 2022 as oral presentation.</li> <li>[Dec. 2021] Our work on general facial representation learning named <strong>FaRL</strong> is released, code available on <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">Github<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[Jul. 2021] Our work on Deepfake Detection accepted by ICCV 2021.</li></ul> <h2 id="publications">Publications</h2> <div class="md-card"><div class="card-image"><img src="https://s1.ax1x.com/2022/08/29/vfir5D.png" alt></div> <div class="card-content"><p>MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining</p> <p>Xiaoyi Dong, Jianmin Bao, <strong>Yinglin Zheng</strong>, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu</p> <p>2023, IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</p> <p><a href="https://arxiv.org/abs/2208.12262" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/musicface.png" alt></div> <div class="card-content"><p>MusicFace: Music-driven Expressive Singing Face Synthesis</p> <p>Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, <strong>Yinglin Zheng</strong>, Yiwei Ding, Xiaohu Guo, Ming Zeng*</p> <p>2023, Computational Visual Media(CVMJ 2023)</p> <p><a href="https://vcg.xmu.edu.cn/datasets/singingface/index.html" target="_blank" rel="noopener noreferrer">Dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/emef.png" alt></div> <div class="card-content"><p>EMEF: Ensemble Multi-Exposure Image Fusion</p> <p>Renshuai Liu, Chengyang Li, Haitao Cao, <strong>Yinglin Zheng</strong>, Ming Zeng, Xuan Cheng*</p> <p>2023, AAAI Conference on Artificial Intelligence (AAAI), Oral presentation.</p> <p><a href="https://github.com/medalwill/EMEF" target="_blank" rel="noopener noreferrer">Project<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://s1.ax1x.com/2022/04/21/LyGJKK.png" alt></div> <div class="card-content"><p>I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation</p> <p>Yiwei Ding, Wenjin Deng, <strong>Yinglin Zheng</strong>, Pengfei Liu, Jianmin Bao, Meihong Wang, Xuan Cheng, Ming Zeng, Dong Chen</p> <p>2022, The 31st International Joint Conference on Artificial Intelligence (IJCAI-22)</p> <p><a href="https://arxiv.org/abs/2206.10892" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/leijue222/Intra-and-Inter-Human-Relation-Network-for-MPEE" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://s4.ax1x.com/2021/12/08/oRXqhj.png" alt></div> <div class="card-content"><p>General Facial Representation Learning in a Visual-Linguistic Manner</p> <p><strong>Yinglin Zheng</strong>, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, Fang Wen</p> <p>2022, IEEE Conference on Computer Vision and Pattern Recognition(CVPR), Oral presentation.</p> <p><a href="https://arxiv.org/abs/2112.03109" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/FacePerceiver/LAION-Face" target="_blank" rel="noopener noreferrer">Dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/ftcn.png" alt></div> <div class="card-content"><p>Exploring Temporal Coherence for More General Video Face Forgery Detection</p> <p><strong>Yinglin Zheng</strong>, Jianmin Bao, Dong Chen, Ming Zeng, Fang Wen</p> <p>2021, International Conference on Computer Vision(ICCV)</p> <p><a href="https://arxiv.org/abs/2108.06693" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/yinglinzheng/FTCN" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/mask_face.png" alt></div> <div class="card-content"><p>Real-time Masked Face Revealing for Video Conference</p> <p>Jinpeng Lin, Pengfei Liu, <strong>Yinglin Zheng</strong>, Wenjin Deng, Ming Zeng</p> <p>2021, IEEE International Conference on Multimedia and Expo (ICME), Oral presentation.</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/uv_iccv.png" alt></div> <div class="card-content"><p>UV空间中的人脸颜色纹理和几何细节协同补全</p> <p>程轩，刘仁帅，<strong>郑英林</strong>，曾鸣</p> <p>2020, Chinagraph</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/vh3d.png" alt></div> <div class="card-content"><p>VH3D-LSFM:Video-based Human 3D Pose Estimation with Long-term and Short-term Pose Fusion Mechanism</p> <p>Wenjin Deng, <strong>Yinglin Zheng</strong>, Hui Li, Xianwei Wang, Zizhao Wu, Ming Zeng</p> <p>2020, Chinese Conference on Pattern Recognition and Computer Vision(PRCV)</p> <p><a href="https://www.researchgate.net/publication/346167722_VH3D-LSFM_Video-Based_Human_3D_Pose_Estimation_with_Long-Term_and_Short-Term_Pose_Fusion_Mechanism" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/denoise.png" alt></div> <div class="card-content"><p>Spatially Adaptive Regularizer for Mesh Denoising</p> <p>Xuan Cheng, <strong>Yinglin Zheng</strong>, Yuhui Zheng, Fang Chen, Kunhui Lin</p> <p>2020, IEEE Access</p> <p><a href="https://www.researchgate.net/publication/340572393_Spatially_Adaptive_Regularizer_for_Mesh_Denoising" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/joint.png" alt></div> <div class="card-content"><p>Joint Depth-Face Translation and Facial Alignment via Multi-task Learning</p> <p>2019, Multimedia Tools and Applications</p> <p>Xiaoli Wang, <strong>Yinglin Zheng</strong>, Ming Zeng, Xuan Cheng, Wei Lu.</p> <p><a href="https://www.researchgate.net/publication/341411157_Joint_learning_for_face_alignment_and_face_transfer_with_depth_image" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <h2 id="awards-honors">Awards &amp; Honors</h2> <ul><li>National Scholarship for Graduate Students, 2022</li> <li>Stars of Tomorrow (Award of Excellent Intern), Microsoft Research Asia, 2022</li> <li><strong>Rank 10</strong> among 2265 teams in <strong>Kaggle Deepfake Detection Challenge</strong>.</li> <li><strong>Champion</strong> of 3D Face Alignment in the Wild Challenge (In conjunction with ICCV 2019), Seoul, Korea, 2019.</li></ul> <h2 id="education-experiences">Education &amp; Experiences</h2> <ul><li><p><strong>Research Intern, Visual Computing Group, Microsoft Research Asia</strong> <br>
Dec. 2022 - Mar. 2024</p></li> <li><p><strong>Research Intern, Visual Computing Group, Microsoft Research Asia</strong> <br>
Sep. 2020 - Mar. 2022</p></li> <li><p><strong>Doctoral Student of Computer Science, School of Informatics, Xiamen University</strong> <br>
Sep. 2020 - present</p></li> <li><p><strong>Bachelor of Software Engineering, School of Informatics, Xiamen University</strong> <br>
Sep. 2016 - Jun. 2020</p></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.9223913a.js" defer></script><script src="/assets/js/2.51cc0459.js" defer></script><script src="/assets/js/9.5892c46c.js" defer></script><script src="/assets/js/4.905fbb11.js" defer></script><script src="/assets/js/5.98d9baeb.js" defer></script>
  </body>
</html>
